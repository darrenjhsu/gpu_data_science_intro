{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6d7e8e-778e-42bb-97d0-d0f559cbf291",
   "metadata": {},
   "source": [
    "# Advanced Topics\n",
    "\n",
    "## Working with data that doesn't fit into GPU memory\n",
    "\n",
    "You've probably worked with data that is too large to fit into GPU or even system memory in the past.\n",
    "In this section, we're going to explore methods to work with data that is larger than GPU memory.\n",
    "\n",
    "### Quickly generating data\n",
    "\n",
    "Since we're going to be generating larger datasets, we're also going to use a `mp_generate_data` from [extras.py](extras.py).\n",
    "If you look at the code, you'll notice that it not only is multi-core, but also generates the data in FP32.\n",
    "\n",
    "Run the code below to test out the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe81b694-3730-4e1e-8bfc-7d45232d9f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000000 points\n",
      " - original (FP64) - 0.15 sec - 30.52 MB\n",
      " - original (FP32) - 0.36 sec - 15.26 MB\n",
      "Generating 10000000 points\n",
      " - original (FP64) - 1.52 sec - 305.18 MB\n",
      " - original (FP32) - 0.49 sec - 152.59 MB\n",
      "Generating 100000000 points\n",
      " - original (FP64) - 15.18 sec - 3051.76 MB\n",
      " - original (FP32) - 2.03 sec - 1525.88 MB\n"
     ]
    }
   ],
   "source": [
    "from extras import generate_data, mp_generate_data\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**power for power in range(6,9)]:\n",
    "    # Testing the origninal generate_data\n",
    "    print(f\"Generating {N} points\")\n",
    "    start_time = time()\n",
    "    data, real_centroids = generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    elapsed_orig = time()-start_time\n",
    "    data_mb = np.round(data.size * data.itemsize / 2**20, 3)\n",
    "    print(f\" - original (FP64) - {elapsed_orig:2.2f} sec - {data_mb:4.2f} MB\")\n",
    "\n",
    "    # Testing mp_generate_data\n",
    "    start_time = time()\n",
    "    data, real_centroids = mp_generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    elapsed_orig = time()-start_time\n",
    "    data_mb = np.round(data.size * data.itemsize / 2**20, 3)\n",
    "    print(f\" - original (FP32) - {elapsed_orig:2.2f} sec - {data_mb:4.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836c62e-075f-4a38-ba4e-8b73c159f219",
   "metadata": {},
   "source": [
    "Looking at the results, `mp_generate_data` is worth using when generating 10,000,000 or more data points.\n",
    "The FP32 values it generates are also not only more suited to running on the GPU, but they take up less memory as well (at the expense of precision).\n",
    "\n",
    "### Testing the limits of GPU memory\n",
    "\n",
    "Now that we can generate data quickly, we can start exploring the limits of GPU-accelerated algorithms.\n",
    "\n",
    "> This was developed on a T4 with 16GB of VRAM. Increase the data size if 400M points does not fail on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd7d21f-3afa-4e48-8215-3bad02d61295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100000000 points (1525.879 MB)\n",
      "  - sklearn 41.7196 seconds\n",
      "  - cuml 4.9657 seconds\n",
      "Generated 400000000 points (6103.516 MB)\n",
      "  - sklearn 165.8098 seconds\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc: out_of_memory: CUDA error at: /usr/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorMemoryAllocation out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - sklearn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.round(elapsed_time,\u001b[32m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m start_time = time()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m assignments = \u001b[43mcuml_KMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_init\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m elapsed_time = time()-start_time\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - cuml \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.round(elapsed_time,\u001b[32m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:193\u001b[39m, in \u001b[36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     set_api_output_dtype(output_dtype)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m process_return:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:416\u001b[39m, in \u001b[36menable_device_interop.<locals>.dispatch\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdispatch_func\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    415\u001b[39m     func_name = gpu_func.\u001b[34m__name__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatch_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m gpu_func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:195\u001b[39m, in \u001b[36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m         ret = func(*args, **kwargs)\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cm.process_return(ret)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mbase.pyx:764\u001b[39m, in \u001b[36mcuml.internals.base.UniversalBase.dispatch_func\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mkmeans.pyx:470\u001b[39m, in \u001b[36mcuml.cluster.kmeans.KMeans.fit_predict\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:193\u001b[39m, in \u001b[36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     set_api_output_dtype(output_dtype)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m process_return:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:416\u001b[39m, in \u001b[36menable_device_interop.<locals>.dispatch\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdispatch_func\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    415\u001b[39m     func_name = gpu_func.\u001b[34m__name__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatch_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m gpu_func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:195\u001b[39m, in \u001b[36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m         ret = func(*args, **kwargs)\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cm.process_return(ret)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mbase.pyx:764\u001b[39m, in \u001b[36mcuml.internals.base.UniversalBase.dispatch_func\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mkmeans.pyx:391\u001b[39m, in \u001b[36mcuml.cluster.kmeans.KMeans.fit\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: std::bad_alloc: out_of_memory: CUDA error at: /usr/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorMemoryAllocation out of memory"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans as skl_KMeans\n",
    "from cuml.cluster import KMeans as cuml_KMeans\n",
    "from extras import generate_data, mp_generate_data\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**8, 4*10**8]:# for power in range(7,10)]:\n",
    "    data, real_centroids = mp_generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    data_mb = np.round(data.size * data.itemsize / 2**20, 3)\n",
    "    print(f\"Generated {N} points ({data_mb} MB)\")\n",
    "    \n",
    "    start_time = time()\n",
    "    assignments = skl_KMeans(n_clusters=k, random_state=0, max_iter=1).fit_predict(data)\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"  - sklearn {np.round(elapsed_time,4)} seconds\")\n",
    "    \n",
    "    start_time = time()\n",
    "    assignments = cuml_KMeans(n_clusters=k, random_state=0, n_init='auto', max_iter=1).fit_predict(data)\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"  - cuml {np.round(elapsed_time,4)} seconds\")\n",
    "    del data, real_centroids, assignments\n",
    "print(\"Done\") # Tell me when to stop waiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c52acd-bbc8-4961-93ba-95541aad8dad",
   "metadata": {},
   "source": [
    "You can see that GPU-based KMeans failed on the second data set because it ran out of memory.\n",
    "While slower, the CPU-based scikit-learn KMeans version completed because it ran from (the usually larger) system memory (RAM).\n",
    "\n",
    "### RAPIDS Memory Manager\n",
    "\n",
    "While it may see like the end of the road, the [RAPIDS Memory Manager](https://docs.rapids.ai/api/rmm/stable/) (RMM) was created help allocate GPU memory in highly configurable ways.\n",
    "One of those ways is to use Unified Virtual Memory (UVM) provides a single, unified memory address space accessible from both the CPU and GPU, simplifying memory management and enabling efficient data sharing between processors.\n",
    "UVM also allows data in system (CPU) RAM to be larger than GPU memory, and handles all the transactions necessary when computing on it.\n",
    "\n",
    "UVM can be enabled by setting `managed_memory=True` when initializing RMM.\n",
    "Do this and try re-running cuML KMeans that previously failed due to OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79d6001c-6078-4623-aa28-effc63c9b8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100000000 points (1525.879 MB)\n",
      "  - cuml 4.4727 seconds\n",
      "Generated 400000000 points (6103.516 MB)\n",
      "  - cuml 31.2132 seconds\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import rmm\n",
    "# Enabled managed memory (allows spilling)\n",
    "rmm.reinitialize(managed_memory=True)\n",
    "from cuml.cluster import KMeans as cuml_KMeans\n",
    "from extras import mp_generate_data\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**8, 4*10**8]:# for power in range(7,10)]:\n",
    "    data, real_centroids = mp_generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    data_mb = np.round(data.size * data.itemsize / 2**20, 3)\n",
    "    print(f\"Generated {N} points ({data_mb} MB)\")\n",
    "\n",
    "    # Skip CPU since it works\n",
    "    \n",
    "    start_time = time()\n",
    "    assignments = cuml_KMeans(n_clusters=k, random_state=0, n_init='auto', max_iter=1).fit_predict(data)\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"  - cuml {np.round(elapsed_time,4)} seconds\")\n",
    "    del data, real_centroids, assignments\n",
    "print(\"Done\") # I sometimes wait for another step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c321f-db91-498d-89e0-548bccefd74d",
   "metadata": {},
   "source": [
    "You can see that there is a performance hit, but it does complete without errors and no longer limits your computation to data that fits in GPU memory.\n",
    "\n",
    "### Exercises:\n",
    "\n",
    "- Try larger datasets to see if there's a failure point\n",
    "\n",
    "## Using Multiple GPUs\n",
    "\n",
    "For multiple GPUs, we recommend [Dask](https://docs.dask.org/en/stable/) and [dask-cuda](https://docs.rapids.ai/api/dask-cuda/nightly/quickstart/). Dask is an open-source Python library that enables parallel and distributed computing for large-scale data processing, providing advanced data structures and algorithms that scale from single machines to large clusters.\n",
    "dask-cuda takes this a step further by doing this with GPU data structures like CuPy arrays and cuDF dataframes for scaling computations to multiple GPUs and GPU-nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f49414f9-42d2-49fb-ae14-a13627f24015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100000000 points (1525.879 MB)\n",
      "  - dask+cuml 11.2605 seconds\n",
      "Generated 400000000 points (6103.516 MB)\n",
      "  - dask+cuml 25.7553 seconds\n",
      "Done\n",
      "165118976\n",
      "895356928\n",
      "234881024\n",
      "704643072\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import dask\n",
    "# Specify that the dask cluster will use cupy (gpu-numpy) for arrays\n",
    "dask.config.set({\"array.backend\": \"cupy\", \"logging.distributed\": \"error\"})\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "# Import dask version of KMeans\n",
    "from cuml.dask.cluster import KMeans as cuml_KMeans\n",
    "\n",
    "# Create a local (single-node) cluster and used RMM managed memory (OOM otherwise)\n",
    "cluster = LocalCUDACluster(rmm_managed_memory=True, rmm_pool_size='15GB')\n",
    "# Connect to the cluster\n",
    "client = Client(cluster)\n",
    "\n",
    "# Generate data as dask arrays\n",
    "def dask_generate_data(k=3, n=10, d=2, cmin=0, cmax=10):\n",
    "    # Make sure to specify float32!\n",
    "    C = da.random.random((k,d), dtype=cp.float32)*(cmax-cmin)-cmin\n",
    "    R = da.random.normal(loc=0, scale=1, size=(n, d), dtype=cp.float32)+C[da.random.choice(k, n, replace=True)]\n",
    "    return R, C\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**8, 4*10**8]:# for power in range(7,10)]:\n",
    "    dask_data, real_centroids = dask_generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    data_mb = np.round(dask_data.size * dask_data.itemsize / 2**20, 3)\n",
    "    print(f\"Generated {N} points ({data_mb} MB)\")\n",
    "    \n",
    "    start_time = time()\n",
    "    assignments = cuml_KMeans(n_clusters=k, random_state=0, n_init='auto', max_iter=1).fit_predict(dask_data)\n",
    "    assignments.compute()\n",
    "    #print(assignments.map_blocks(cp.asnumpy).compute()[:10])\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"  - dask+cuml {np.round(elapsed_time,4)} seconds\")\n",
    "    del dask_data, real_centroids, assignments\n",
    "print(\"Done\") # I sometimes wait for another step\n",
    "\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe20e1-bda1-49f0-8af4-3a53c51244d5",
   "metadata": {},
   "source": [
    "You'll notice that using Dask on two T4s (no NVLink) was actually slower for a 1.5GB dataset, but faster for the 6GB scale.\n",
    "Whenever an algorithm is scaled to multiple workers, there's overhead from communication that limits the speed.\n",
    "You saw this with `mp_generate_data` and now with Dask.\n",
    "\n",
    "### Exercises:\n",
    "\n",
    "- Try running on more than two GPUs\n",
    "    - What does the scaling efficiency look like?\n",
    "- What happens if you exclude RMM?\n",
    "- What happens if you exclude the pool size?\n",
    "- Try [enabling UCX communication](https://docs.rapids.ai/api/dask-cuda/stable/examples/ucx/) on a system with NVLink\n",
    "- What scale of data does using Dask for multiple-GPUs make sense on your system?\n",
    "\n",
    "> Make sure to restart the kernel between runs\n",
    "\n",
    "### Additional Dask features\n",
    "\n",
    "Dask is also capable of scaling to GPUs on multiple nodes.\n",
    "We're not going to cover this topic today, but take a look at their [cluster deployment documentation](https://docs.dask.org/en/stable/deploying.html#) to figure out which method is suitable for your compute infrastructure.\n",
    "\n",
    "In addition to scaling algorithms, Dask commonly known for splitting large data objects (chunking) and persisting chunks in different tiers of memory (spilling).\n",
    "There's support for this at the GPU level (without RMM), but has best support with [cuDF data frames](https://docs.rapids.ai/api/dask-cudf/stable/). Some RAPIDS alorithms (like Kmeans) also may not work well with this feature.\n",
    "\n",
    "## Topics not covered, but worth exploring\n",
    "\n",
    "- [Numba](https://numba.readthedocs.io/en/stable/cuda/index.html) - Numba is a compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python.\n",
    "- [NVIDIA cuPyNumeric](https://docs.nvidia.com/cupynumeric/latest/index.html) - NVIDIA-developed distributed and accelerated drop-in replacement for NumPy built on top of the Legate framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599aedf0-c4c6-4b26-9171-8a9d752b195b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
