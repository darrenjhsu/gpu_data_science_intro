{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6d7e8e-778e-42bb-97d0-d0f559cbf291",
   "metadata": {},
   "source": [
    "# Advanced Topics\n",
    "\n",
    "## Working with data that doesn't fit into GPU memory\n",
    "\n",
    "You've probably worked with data that is too large to fit into GPU or even system memory in the past.\n",
    "In this section, we're going to explore methods to work with data that is larger than GPU memory.\n",
    "\n",
    "### Quickly generating data\n",
    "\n",
    "Since we're going to be generating larger datasets, we're also going to use a `mp_generate_data` from [extras.py](extras.py).\n",
    "If you look at the code, you'll notice that it not only is multi-core, but also generates the data in FP32.\n",
    "\n",
    "Run the code below to test out the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81b694-3730-4e1e-8bfc-7d45232d9f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extras import generate_data, mp_generate_data\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**power for power in range(6,9)]:\n",
    "    # Testing the origninal generate_data\n",
    "    print(f\"Generating {N} points\")\n",
    "    start_time = time()\n",
    "    data, real_centroids = generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    elapsed_orig = time()-start_time\n",
    "    data_mb = np.round(data.size * data.itemsize / 2**20, 3)\n",
    "    print(f\" - original  (FP64) - {elapsed_orig:5.2f} sec - {data_mb:4.2f} MB\")\n",
    "\n",
    "    # Testing mp_generate_data\n",
    "    start_time = time()\n",
    "    data, real_centroids = mp_generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    elapsed_orig = time()-start_time\n",
    "    data_mb = np.round(data.size * data.itemsize / 2**20, 3)\n",
    "    print(f\" - multiproc (FP32) - {elapsed_orig:5.2f} sec - {data_mb:4.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836c62e-075f-4a38-ba4e-8b73c159f219",
   "metadata": {},
   "source": [
    "Looking at the results, `mp_generate_data` is worth using when generating 10,000,000 or more data points.\n",
    "The FP32 values it generates are also not only more suited to running on the GPU, but they take up less memory as well (at the expense of precision).\n",
    "\n",
    "### Testing the limits of GPU memory\n",
    "\n",
    "Now that we can generate data quickly, we can start exploring the limits of GPU-accelerated algorithms.\n",
    "\n",
    "> This was developed on a T4 with 16GB of VRAM. Increase the data size if 400M points does not fail on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd7d21f-3afa-4e48-8215-3bad02d61295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100000000 points (1525.879 MB)\n",
      "  - cuml 5.7894 seconds\n",
      "Generated 800000000 points (12207.031 MB)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc: out_of_memory: CUDA error at: /usr/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorMemoryAllocation out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#start_time = time()\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#assignments = skl_KMeans(n_clusters=k, random_state=0, max_iter=1).fit_predict(data)\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#elapsed_time = time()-start_time\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m#print(f\"  - sklearn {np.round(elapsed_time,4)} seconds\")\u001b[39;00m\n\u001b[32m     19\u001b[39m start_time = time()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m assignments = \u001b[43mcuml_KMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_init\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m elapsed_time = time()-start_time\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - cuml \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.round(elapsed_time,\u001b[32m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:193\u001b[39m, in \u001b[36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     set_api_output_dtype(output_dtype)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m process_return:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:416\u001b[39m, in \u001b[36menable_device_interop.<locals>.dispatch\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdispatch_func\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    415\u001b[39m     func_name = gpu_func.\u001b[34m__name__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatch_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m gpu_func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:195\u001b[39m, in \u001b[36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m         ret = func(*args, **kwargs)\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cm.process_return(ret)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mbase.pyx:764\u001b[39m, in \u001b[36mcuml.internals.base.UniversalBase.dispatch_func\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mkmeans.pyx:470\u001b[39m, in \u001b[36mcuml.cluster.kmeans.KMeans.fit_predict\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:193\u001b[39m, in \u001b[36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     set_api_output_dtype(output_dtype)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m process_return:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:416\u001b[39m, in \u001b[36menable_device_interop.<locals>.dispatch\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdispatch_func\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    415\u001b[39m     func_name = gpu_func.\u001b[34m__name__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatch_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m gpu_func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/api_decorators.py:195\u001b[39m, in \u001b[36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m         ret = func(*args, **kwargs)\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cm.process_return(ret)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mbase.pyx:764\u001b[39m, in \u001b[36mcuml.internals.base.UniversalBase.dispatch_func\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mkmeans.pyx:404\u001b[39m, in \u001b[36mcuml.cluster.kmeans.KMeans.fit\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: std::bad_alloc: out_of_memory: CUDA error at: /usr/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorMemoryAllocation out of memory"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans as skl_KMeans\n",
    "from cuml.cluster import KMeans as cuml_KMeans\n",
    "from extras import generate_data, mp_generate_data\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**8, 8*10**8]:# for power in range(7,10)]:\n",
    "    data, real_centroids = mp_generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    data_mb = np.round(data.size * data.itemsize / 2**20, 3)\n",
    "    print(f\"Generated {N} points ({data_mb} MB)\")\n",
    "    \n",
    "    #start_time = time()\n",
    "    #assignments = skl_KMeans(n_clusters=k, random_state=0, max_iter=1).fit_predict(data)\n",
    "    #elapsed_time = time()-start_time\n",
    "    #print(f\"  - sklearn {np.round(elapsed_time,4)} seconds\")\n",
    "    \n",
    "    start_time = time()\n",
    "    assignments = cuml_KMeans(n_clusters=k, random_state=0, n_init='auto', max_iter=1).fit_predict(data)\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"  - cuml {np.round(elapsed_time,4)} seconds\")\n",
    "    del data, real_centroids, assignments\n",
    "print(\"Done\") # Tell me when to stop waiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c52acd-bbc8-4961-93ba-95541aad8dad",
   "metadata": {},
   "source": [
    "You can see that GPU-based KMeans failed on the second data set because it ran out of memory.\n",
    "While slower, the CPU-based scikit-learn KMeans version completed because it ran from (the usually larger) system memory (RAM).\n",
    "\n",
    "### RAPIDS Memory Manager\n",
    "\n",
    "While it may see like the end of the road, the [RAPIDS Memory Manager](https://docs.rapids.ai/api/rmm/stable/) (RMM) was created help allocate GPU memory in highly configurable ways.\n",
    "One of those ways is to use Unified Virtual Memory (UVM) provides a single, unified memory address space accessible from both the CPU and GPU, simplifying memory management and enabling efficient data sharing between processors.\n",
    "UVM also allows data in system (CPU) RAM to be larger than GPU memory, and handles all the transactions necessary when computing on it.\n",
    "\n",
    "UVM can be enabled by setting `managed_memory=True` when initializing RMM.\n",
    "Do this and try re-running cuML KMeans that previously failed due to OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79d6001c-6078-4623-aa28-effc63c9b8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100000000 points (1525.879 MB)\n",
      "  - cuml 6.2042 seconds\n",
      "Generated 800000000 points (12207.031 MB)\n",
      "  - cuml 67.8275 seconds\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import rmm\n",
    "# Enabled managed memory (allows spilling)\n",
    "rmm.reinitialize(managed_memory=True)\n",
    "from cuml.cluster import KMeans as cuml_KMeans\n",
    "from extras import mp_generate_data\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**8, 8*10**8]:# for power in range(7,10)]:\n",
    "    data, real_centroids = mp_generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    data_mb = np.round(data.size * data.itemsize / 2**20, 3)\n",
    "    print(f\"Generated {N} points ({data_mb} MB)\")\n",
    "\n",
    "    # Skip CPU since it works\n",
    "    \n",
    "    start_time = time()\n",
    "    assignments = cuml_KMeans(n_clusters=k, random_state=0, n_init='auto', max_iter=1).fit_predict(data)\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"  - cuml {np.round(elapsed_time,4)} seconds\")\n",
    "    del data, real_centroids, assignments\n",
    "print(\"Done\") # I sometimes wait for another step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c321f-db91-498d-89e0-548bccefd74d",
   "metadata": {},
   "source": [
    "You can see that there is a performance hit, but it does complete without errors and no longer limits your computation to data that fits in GPU memory.\n",
    "\n",
    "### Exercises:\n",
    "\n",
    "- Try larger datasets to see if there's a failure point\n",
    "  - Make sure to look at system memory (RAM)\n",
    "\n",
    "## Using Multiple GPUs\n",
    "\n",
    "For multiple GPUs, we recommend [Dask](https://docs.dask.org/en/stable/) and [dask-cuda](https://docs.rapids.ai/api/dask-cuda/nightly/quickstart/). Dask is an open-source Python library that enables parallel and distributed computing for large-scale data processing, providing advanced data structures and algorithms that scale from single machines to large clusters.\n",
    "dask-cuda takes this a step further by doing this with GPU data structures like CuPy arrays and cuDF dataframes for scaling computations to multiple GPUs and GPU-nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49414f9-42d2-49fb-ae14-a13627f24015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100000000 points (1525.879 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "double free or corruption (!prev)\n",
      "double free or corruption (!prev)\n",
      "2025-06-17 12:19:06,665 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:41839 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://127.0.0.1:35086 remote=tcp://127.0.0.1:41839>: Stream is closed\n",
      "2025-06-17 12:19:06,719 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:39269 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://127.0.0.1:49242 remote=tcp://127.0.0.1:39269>: Stream is closed\n"
     ]
    },
    {
     "ename": "CommClosedError",
     "evalue": "in <TCP (closed) Scheduler Broadcast local=tcp://127.0.0.1:35086 remote=tcp://127.0.0.1:41839>: Stream is closed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCommClosedError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m points (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m start_time = time()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m assignments = \u001b[43mcuml_KMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_init\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdask_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m assignments.compute()\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m#print(assignments.map_blocks(cp.asnumpy).compute()[:10])\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/dask/cluster/kmeans.py:202\u001b[39m, in \u001b[36mKMeans.fit_predict\u001b[39m\u001b[34m(self, X, sample_weight, delayed)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, delayed=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    188\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03m    Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[32m    190\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m \n\u001b[32m    201\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m.predict(\n\u001b[32m    203\u001b[39m         X, sample_weight=sample_weight, delayed=delayed\n\u001b[32m    204\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/internals/memory_utils.py:87\u001b[39m, in \u001b[36mwith_cupy_rmm.<locals>.cupy_rmm_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m GPU_ENABLED:\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m cupy_using_allocator(rmm_cupy_allocator):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/cuml/dask/cluster/kmeans.py:159\u001b[39m, in \u001b[36mKMeans.fit\u001b[39m\u001b[34m(self, X, sample_weight)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# This needs to happen on the scheduler\u001b[39;00m\n\u001b[32m    158\u001b[39m comms = Comms(comms_p2p=\u001b[38;5;28;01mFalse\u001b[39;00m, client=\u001b[38;5;28mself\u001b[39m.client)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[43mcomms\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m kmeans_fit = [\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mself\u001b[39m.client.submit(\n\u001b[32m    163\u001b[39m         KMeans._func_fit,\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, wf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data.worker_to_parts.items())\n\u001b[32m    173\u001b[39m ]\n\u001b[32m    175\u001b[39m wait_and_raise_from_futures(kmeans_fit)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/raft_dask/common/comms.py:200\u001b[39m, in \u001b[36mComms.init\u001b[39m\u001b[34m(self, workers)\u001b[39m\n\u001b[32m    196\u001b[39m worker_info = {w: worker_info[w] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.worker_addresses}\n\u001b[32m    198\u001b[39m \u001b[38;5;28mself\u001b[39m.create_nccl_uniqueid()\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_func_init_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msessionId\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muniqueId\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcomms_p2p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworker_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstreams_per_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworker_addresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m.nccl_initialized = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.comms_p2p:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/distributed/client.py:3193\u001b[39m, in \u001b[36mClient.run\u001b[39m\u001b[34m(self, function, workers, wait, nanny, on_error, *args, **kwargs)\u001b[39m\n\u001b[32m   3110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\n\u001b[32m   3111\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3112\u001b[39m     function,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3118\u001b[39m     **kwargs,\n\u001b[32m   3119\u001b[39m ):\n\u001b[32m   3120\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m    Run a function on all workers outside of task scheduling system\u001b[39;00m\n\u001b[32m   3122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3191\u001b[39m \u001b[33;03m    >>> c.run(print_state, wait=False)  # doctest: +SKIP\u001b[39;00m\n\u001b[32m   3192\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3196\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnanny\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnanny\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3201\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3202\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/distributed/utils.py:363\u001b[39m, in \u001b[36mSyncMethodMixin.sync\u001b[39m\u001b[34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m future\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/distributed/utils.py:439\u001b[39m, in \u001b[36msync\u001b[39m\u001b[34m(loop, func, callback_timeout, *args, **kwargs)\u001b[39m\n\u001b[32m    436\u001b[39m         wait(\u001b[32m10\u001b[39m)\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/distributed/utils.py:413\u001b[39m, in \u001b[36msync.<locals>.f\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    411\u001b[39m         awaitable = wait_for(awaitable, timeout)\n\u001b[32m    412\u001b[39m     future = asyncio.ensure_future(awaitable)\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     result = \u001b[38;5;28;01myield\u001b[39;00m future\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m    415\u001b[39m     error = exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/tornado/gen.py:766\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    764\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m         value = \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    767\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    768\u001b[39m         \u001b[38;5;66;03m# Save the exception for later. It's important that\u001b[39;00m\n\u001b[32m    769\u001b[39m         \u001b[38;5;66;03m# gen.throw() not be called inside this try/except block\u001b[39;00m\n\u001b[32m    770\u001b[39m         \u001b[38;5;66;03m# because that makes sys.exc_info behave unexpectedly.\u001b[39;00m\n\u001b[32m    771\u001b[39m         exc: Optional[\u001b[38;5;167;01mException\u001b[39;00m] = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/distributed/client.py:3098\u001b[39m, in \u001b[36mClient._run\u001b[39m\u001b[34m(self, function, nanny, workers, wait, on_error, *args, **kwargs)\u001b[39m\n\u001b[32m   3095\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   3097\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3098\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m   3099\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m on_error == \u001b[33m\"\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3100\u001b[39m     results[key] = exc\n",
      "\u001b[31mCommClosedError\u001b[39m: in <TCP (closed) Scheduler Broadcast local=tcp://127.0.0.1:35086 remote=tcp://127.0.0.1:41839>: Stream is closed"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import dask\n",
    "# Specify that the dask cluster will use cupy (gpu-numpy) for arrays\n",
    "dask.config.set({\"array.backend\": \"cupy\", \"logging.distributed\": \"error\"})\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "# Import dask version of KMeans\n",
    "from cuml.dask.cluster import KMeans as cuml_KMeans\n",
    "\n",
    "# Create a local (single-node) cluster and used RMM managed memory (OOM otherwise)\n",
    "cluster = LocalCUDACluster(rmm_managed_memory=True, rmm_pool_size='15GB')\n",
    "# Connect to the cluster\n",
    "client = Client(cluster)\n",
    "\n",
    "# Generate data as dask arrays\n",
    "def dask_generate_data(k=3, n=10, d=2, cmin=0, cmax=10):\n",
    "    # Make sure to specify float32!\n",
    "    C = da.random.random((k,d), dtype=cp.float32)*(cmax-cmin)-cmin\n",
    "    R = da.random.normal(loc=0, scale=1, size=(n, d), dtype=cp.float32)+C[da.random.choice(k, n, replace=True)]\n",
    "    return R, C\n",
    "\n",
    "k, d = 4, 4\n",
    "\n",
    "for N in [10**8, 8*10**8]:# for power in range(7,10)]:\n",
    "    dask_data, real_centroids = dask_generate_data(k=k, n=N, d=d, cmin=0, cmax=100)\n",
    "    data_mb = np.round(dask_data.size * dask_data.itemsize / 2**20, 3)\n",
    "    print(f\"Generated {N} points ({data_mb} MB)\")\n",
    "    \n",
    "    start_time = time()\n",
    "    assignments = cuml_KMeans(n_clusters=k, random_state=0, n_init='auto', max_iter=1).fit_predict(dask_data)\n",
    "    assignments.compute()\n",
    "    #print(assignments.map_blocks(cp.asnumpy).compute()[:10])\n",
    "    elapsed_time = time()-start_time\n",
    "    print(f\"  - dask+cuml {np.round(elapsed_time,4)} seconds\")\n",
    "    del dask_data, real_centroids, assignments\n",
    "print(\"Done\") # I sometimes wait for another step\n",
    "\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe20e1-bda1-49f0-8af4-3a53c51244d5",
   "metadata": {},
   "source": [
    "You'll notice that using Dask on two T4s (no NVLink) was actually slower for a 1.5GB dataset, but faster for the 6GB scale.\n",
    "Whenever an algorithm is scaled to multiple workers, there's overhead from communication that limits the speed.\n",
    "You saw this with `mp_generate_data` and now with Dask.\n",
    "\n",
    "### Exercises:\n",
    "\n",
    "- Try running on more than two GPUs\n",
    "    - What does the scaling efficiency look like?\n",
    "- What happens if you exclude RMM?\n",
    "- What happens if you exclude the pool size?\n",
    "- Try [enabling UCX communication](https://docs.rapids.ai/api/dask-cuda/stable/examples/ucx/) on a system with NVLink\n",
    "- What scale of data does using Dask for multiple-GPUs make sense on your system?\n",
    "\n",
    "> Make sure to restart the kernel between runs\n",
    "\n",
    "### Additional Dask features\n",
    "\n",
    "Dask is also capable of scaling to GPUs on multiple nodes.\n",
    "We're not going to cover this topic today, but take a look at their [cluster deployment documentation](https://docs.dask.org/en/stable/deploying.html#) to figure out which method is suitable for your compute infrastructure.\n",
    "\n",
    "In addition to scaling algorithms, Dask commonly known for splitting large data objects (chunking) and persisting chunks in different tiers of memory (spilling).\n",
    "There's support for this at the GPU level (without RMM), but has best support with [cuDF data frames](https://docs.rapids.ai/api/dask-cudf/stable/). Some RAPIDS alorithms (like Kmeans) also may not work well with this feature.\n",
    "\n",
    "## Topics not covered, but worth exploring\n",
    "\n",
    "- [Numba](https://numba.readthedocs.io/en/stable/cuda/index.html) - Numba is a compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python.\n",
    "- [NVIDIA cuPyNumeric](https://docs.nvidia.com/cupynumeric/latest/index.html) - NVIDIA-developed distributed and accelerated drop-in replacement for NumPy built on top of the Legate framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599aedf0-c4c6-4b26-9171-8a9d752b195b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ngc_pytorch_25.03-py3",
   "language": "python",
   "name": "ngc_pytorch_25.03-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
